[experiment]
tasks = ["wikitext2_line"]
seed = 42
device = "cuda"

[model]
type = "rope"
vocabulary_size = 40479
embedding_size = 512
sequence_length = 255
num_heads = 6
num_layers = 4
rope_theta = 10000.0
intermediate_size = 2048
tie_word_embeddings = true

[data]
batch_size = 8
num_workers = 0
tokenizer_path = "./data/wikitext2_tokenizer.json"

[optimizer]
name = "AdaRMSuon"
lr = 0.0001
betas = [0.9, 0.999]
eps = 1e-08
# The base weight_decay is now controlled by the adaptive mechanism
weight_decay = 0.1 
ns_steps = 5

[train]
epochs = 10
log_every = 10
ckpt_every = 2

[adaptive_wd]
enabled = true
mode = "pcwd"
ema_beta = 0.95
alpha = 0.1
lambda_min = 0.1
lambda_max = 10