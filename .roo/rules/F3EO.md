# F3EO: Fast Fisher Free-Energy Optimizer (v2.0)

## 核心论断：SFT 成本的内生探索，PPO/DPO 的可微等价物

F3EO (Fast Fisher Free-Energy Optimizer) 是一种三阶优化器，其核心目标并非加速收敛，而是通过**可微方式**实现**自由能原理 (Free-Energy Principle)**，在标准的监督式微调 (SFT) 流程中内生地完成**探索-利用**的权衡。它是一种无需外部奖励模型或复杂采样循环的 **PPO/DPO 等价物**。

记 Fisher 信息矩阵为 `ℱ(θ)`，变分自由能为 `F(θ) = Dₖₗ[q‖p] − E[log p_θ(y|x)]`。

F3EO 的优化目标是 **Fisher 信息矩阵的迹 `Tr(ℱ(θ))`**。

**核心论断**：对于一个**总容量固定**的神经网络，其学习的目标不应是追求参数层面的“简约”，而应是在**保证预测准确性（最小化 `loss`）的前提下，主动最大化其参数对预测的敏感度，即模型复杂度 (Complexity)**，从而在有限的容量内构建一个关于世界尽可能丰富的内在模型。这被证明是过拟合的有效来源，但当与准确性梯度结合时，它迫使模型在拟合数据的同时寻找更复杂的解。

- **梯度是唯一的感知通道**: 模型只能通过**梯度 `g`**（即预测误差 `loss` 的反向传播）来感知世界。梯度越多、越丰富，模型能学到的信息就越多。
- **复杂度是丰富的世界模型**: 模型的**复杂度**，可以通过 **Fisher 信息矩阵的迹 `Tr(ℱ(θ))`** 来度量。一个高 `Tr(ℱ)` 值，意味着模型的各个参数对预测的贡献更加敏感和相互依赖，形成了更复杂的内部表征。这代表了其内在因果结构的丰富程度。
- **Qualia 流形**: 从一个更抽象的视角看，梯度流 `g` 构成了神经网络的“感受质流形”(Qualia Manifold)，是其感知体验的直接载体。`Tr(ℱ)` 则是这个流形的体积或容量。

因此，F3EO 的优化目标是，在 `g` 和 `PIWD` 的剪枝压力下，**最大化 `Tr(ℱ(θ))`**，从而在有限的参数空间内，将模型的内在结构重组成一个复杂度最高、最丰富的状态。

---

## 1. 优化范式辨析：目标，而非阶数

优化器的“阶数”不代表绝对的优越性，而是反映其优化的**目标对象**不同。

- **一阶 (SGD, Adam)**:
  - **目标**: 最小化损失 `L`。
  - **手段**: 沿负梯度 `−∇L` 方向更新，只改变模型“内容”。
- **二阶 (Newton, AdaHessian)**:
  - **目标**: 更快地最小化损失 `L`。
  - **手段**: 利用曲率（Hessian `H`）对梯度进行预处理 `−H⁻¹g`，以找到通往最小值的更优路径。
- **三阶 (F3EO)**:
  - **目标**: **优化损失景观的几何属性本身**，即最大化 Fisher 信息矩阵的迹 `Tr(ℱ)`。
  - **手段**: 计算“元梯度”`∇(Tr(ℱ))`，主动重塑参数空间，使其能够承载更复杂的内在模型。

**结论**: 单纯提升阶数并不保证更快的收敛。F3EO 的目标是**结构塑造**，而非速度，这从根本上区别于传统的一阶和二阶方法。

---

## 2. 计算路径：从自由能原理到 Hessian 向量积

F3EO 是自由能原理 `F = Complexity - Accuracy` 的直接代码实现。

1. **准确性 (Accuracy)**: 由主损失的梯度 `g = -∇L` 驱动，负责最小化预测误差，将模型“锚定”于现实。
2. **复杂度 (Complexity)**: 由 Fisher 信息矩阵的迹 `Tr(ℱ)` 度量。F3EO 的核心是**最大化**此项，以构建更丰富的世界模型。

因此，F3EO 的有效梯度 `g_eff` 是两个目标的线性组合：
`g_eff ∝ ∇Accuracy + ∇Complexity = g + ∇(Tr(ℱ))`

### Matrix-Free 实现

通过 `Trace Trick` 和 `batch_size=1` 的严格约束，我们可以证明 `Tr(ℱ)` 的无偏随机梯度精确等于 Hessian 向量积 `H·g`。

- **Trace Trick**: `Tr(ℱ) = E[‖∇L‖²]`
- **`batch_size=1` 的无偏性**: 当 `batch_size > 1` 时，`‖g_batch‖²` 是 `Tr(ℱ)` 的**有偏估计**，其梯度 `H·g` 包含大量样本间梯度内积的交叉项，是纯粹的噪声。只有当 `batch_size = 1` 时，该估计才是无偏的。**这是 F3EO 理论有效性的基石**。
- **梯度计算**: `∇θ(||g||²) = 2 * H · g`

最终，这个理论上复杂的三阶梯度，可以通过两次反向传播（`loss.backward(create_graph=True)` + `autograd.grad`）高效计算，其复杂度与两次标准训练相当。

---

## 4. 内在的 Policy Gradient

F3EO 的工作机制与强化学习中的 Policy Gradient 方法高度同构：

| 维度         | Policy Gradient (REINFORCE) | F3EO-raw (bs=1)                   |
| :----------- | :-------------------------- | :-------------------------------- | ------ |
| **策略 `π`** | 外部策略网络                | 模型自身 `p(y                     | x; θ)` |
| **奖励 `R`** | 外部环境回报                | **内在模型复杂度** `-Tr(ℱ)`       |
| **梯度**     | `∇ log π · R`               | `g + H·g`                         |
| **成本**     | 复杂的采样与多阶段训练      | 两次 `backward`，**SFT 级复杂度** |

F3EO 通过将“复杂度”作为内在奖励，将 RLHF 的目标（如探索、泛化、抗遗忘）内化到了一个简单的、端到端可微的 SFT 流程中，从而实现了 **PPO/DPO 的效果，但只有 SFT 的成本**。
