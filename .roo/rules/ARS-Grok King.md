# ARS-Grok King: 优化器加速 Grokking 现象的实验报告

> **核心结论**: F3EO 自研的 **AdaRMSuon** 与 **ARS (AdaRMSuon + SAM)** 优化器在模加法 (Modular Addition) 任务中，将 Grokking（顿悟）现象的发生时间相比 AdamW 基准提前了 **4 倍以上** (Epoch 228 -> Epoch 54)，有力证明了“能量-几何解耦”与“流形平坦度约束”在加速模型泛化相变中的关键作用。

## 1. 实验背景与设计

本实验旨在研究不同优化器在 **模加法 (Modular Addition)** 任务中触发 **Grokking** 现象的效率与动力学特征。Grokking 指模型在训练集完美拟合后，验证性能长期停滞，随后突然跃升至完美泛化的现象。

### 1.1 任务与模型配置

- **任务**: 模加法 ($a + b \pmod p$)
  - 模数 $p = 113$
  - 数据划分: 训练集占比 30% (`fraction = 0.3`)
  - 代码位置: [`task/mod_addition.py`](task/mod_addition.py)
- **模型**: Grokking Transformer
  - 架构: 1 层 Transformer Block, 4 Heads
  - 维度: $d_{model}=128, d_{mlp}=512$
  - 上下文长度: 3 ($a, b, =$)
- **实验周期**: 最大 15,000 Epochs

### 1.2 对比优化器

实验对比了四种优化器的表现：

1. **AdamW** (基准): 经典的自适应优化器，高权重衰减是触发 Grokking 的标准配置。
2. **Muon**: 专为神经网络隐藏层设计的正交化优化器。
3. **AdaRMSuon** (F3EO 自研): 结合 Adam 的能量统计与 Muon 的几何约束，在黎曼流形上沿测地线滑行。
4. **ARS** (F3EO 自研): **AdaRMSuon + SAM**，在 AdaRMSuon 基础上引入平坦度约束 (Sharpness-Aware Minimization)。

## 2. 实验结果分析

| 优化器  | 拟合速度  | 顿悟时刻| 收敛时刻| 最终性能 | 状态  |
| :-- | :-- | :-- | :-- | :- | :- |
| **AdamW**  | ~Epoch 140 | **Epoch 228** | Epoch 556  | 100.0%| ✅ 标准 Grokking  |
| **Muon**| > Epoch 156 (极慢) | N/A  | N/A  | ~44%  | ❌ 未收敛/失败（等待进一步调参） |
| **AdaRMSuon** | **Epoch 28** | **Epoch 54**  | **Epoch 300** | 99.9% | 🚀 **极速 Grokking** |
| **ARS** | Epoch 17  | **Epoch 100** | Epoch 290  | 99.1% | 🚀 **稳健 Grokking** |

### 2.1 AdamW (基准)

- **动力学特征**: 展现了教科书式的 Grokking 曲线。模型在 Epoch 150 左右完全记忆训练集，随后经历长达 70 个 Epoch 的验证集停滞期，最终在 Epoch 228 开始泛化。
- **结论**: 验证了实验设置的有效性，确立了 Baseline。

### 2.2 Muon

- **动力学特征**: 在此特定任务配置下表现不佳。Train Loss 下降极慢（Epoch 156 时仍 > 4.0），未能有效拟合训练集，因此无法进入 Grokking 阶段。
- **原因推测**: 源于小规模任务上的超参数敏感性，尚待进一步调参。

### 2.3 AdaRMSuon (Chain://Research)

- **动力学特征**:
  - **极速拟合**: 训练集拟合速度比 AdamW 快 5 倍。
  - **提前顿悟**: 验证集准确率在 **Epoch 54** 就开始跃升，几乎在拟合训练集的同时就开始了泛化，大幅缩短了“死记硬背”到“理解规律”的延迟。
- **结论**: 验证了“**能量-几何解耦**”理论的有效性。通过在黎曼流形上沿测地线滑行，模型能以极高效率穿越损失地形。

### 2.4 ARS (Chain://Research)

- **动力学特征**:
  - **平坦度引导**: 引入 SAM 后，虽然泛化时间（Epoch 100）稍晚于纯 AdaRMSuon，但依然远快于 AdamW。
  - **稳健性**: 最终收敛极其稳定。
- **结论**: 证明了在流形优化中引入平坦度约束不会阻碍 Grokking，反而能引导模型进入更平坦、更具稳健性的解区域。

## 总结

实验结果表明，**AdaRMSuon** 是目前加速 Grokking 现象的最强优化器之一。它不仅保留了 Adam 的快速收敛特性，更通过 Muon 的几何约束强制模型在特征空间进行正交化探索，从而避免了在过拟合吸引盆中的无效游走。

**ARS** 的成功进一步表明，**“流形几何 (AdaRMSuon) + 损失地形平坦度 (SAM)”** 是通往高效、稳健泛化的正确路径。
