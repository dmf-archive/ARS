---
title: "Active Sharpening Inference (ASI): 内生锐度感知与动态调度"
category: "优化算法"
status: "🏗️ 架构设计中"
priority: "高"
created: 2026-01-23
owner: "Roo (AI Architect)"
tags: ["ASI", "AGA", "自适应优化", "SAM", "信息论"]
---

## 1. 动机：从外部反馈到内生感知

在 ARS2-Neo 的早期实验中，我们发现固定的扰动半径 ρ 难以平衡训练初期的拟合速度与中后期的泛化性能。Probe Set 方案虽然有效，但引入了验证集信息泄露的风险，且破坏了训练循环的纯粹性。

Active Sharpening Inference (ASI) 演进为一种基于代理间隙（Surrogate Gap）的内生感知机制。它通过监控模型在扰动前后的损失变化，自发地调节系统的“扰动能量”。

> 命名来源： Active Inference in Sharpening Aware Minimization

## 2. 基于代理间隙的反馈逻辑

核心传感器定义为代理间隙 $h_t = L(\theta + \epsilon) - L(\theta)$。该指标直接反映了模型当前所处位置的局部锐度。

### 2.1 趋势分析与调度逻辑

我们采用类似平均真实波动率（ATR）或指数移动平均（EMA）的手法对 $L_t$ 和 $h_t$ 进行平滑处理。

- 协同下降区 ($dL/dt < 0$ 且 $dh/dt < 0$):
  说明模型当前处于平滑且下降的区域。此时应增加 ρ 以注入更多扰动，迫使模型探索更广阔的盆地，防止陷入局部针尖极小值。
- 锐化警示区 ($dL/dt < 0$ 且 $dh/dt > 0$):
  说明模型虽然在降低损失，但代价是进入了更陡峭的区域。此时应显著增加 ρ 并降低 AGA 阈值 λ，强化平坦度约束。
- 过度扰动区 ($dL/dt > 0$):
  说明当前的扰动半径已经破坏了梯度的稳定性，导致损失不降反增。此时应减小 ρ 并适度放宽 λ。

### 2.2 形式化更新规则

引入反馈增益系数 β（可视为元学习步长）：
`rho_{t+1} = clip(rho_t + beta * sign(-dL_t) * sign(-dh_t), rho_min, rho_max)`

这种闭环控制使得 ρ 和 λ 的初始值可以设为 0.0，系统将根据地形自动“生长”出最优的扰动强度。

## 3. 联动 AGA：几何路径的动态修正

ASI 通过干预 AGA 的同步逻辑，在能量（扰动）与几何（路径）两个维度实现闭环：

- 锐度压制: 当 $dh/dt$ 异常升高时，系统自动压低 `aga_lambda`，提高二阶同步频率，确保模型在强扰动下依然沿着正确的测地线滑行。
- 效率平衡: 在平滑区域，系统放宽 `aga_lambda` 进入 Lazy 模式，复用剪切力向量以降低计算开销。

## 4. 语言建模特有的早停判据：内在熵地板

对于 LLM 自回归预测任务，我们引入数据集内在熵 $H(Data)$ 作为优化的理论地板。

### 4.1 理论背景

交叉熵损失可分解为：$L = H(Data) + D_{KL}(P_{data} || P_{model})$。
其中 $H(Data)$ 是数据集不可还原的噪声（Bayes Error Rate）。

### 4.2 应用逻辑

在训练前，通过高阶 N-gram 统计或压缩率预估 $H(Data)$。

- 当 $L_{train}$ 无限接近 $H(Data)$ 时，即使 $L_{eval}$ 仍在微降，ASI 也会判定系统进入“硬刻蚀”阶段。
- 此时系统将强制锁定 ρ 或触发早停，防止模型开始拟合数据集中的随机噪声（如拼写错误或随机标点）。

## 5. 接口设计

### 5.1 优化器配置

- asi_mode: "endogenous"。
- asi_beta: 反馈增益步长。
- asi_ema_alpha: 信号平滑系数。

### 5.2 诊断指标

- surrogate_gap: 实时代理间隙 $h_t$。
- entropy_floor: 预估的数据集内在熵。

## 6. 结论

ASI 将优化器从一个“盲目执行者”转变为一个“自省的探索者”。它不再依赖外部标签泄露，而是通过感知损失地形的几何特性，自适应地在拟合效率与泛化稳健性之间寻找帕累托最优。
