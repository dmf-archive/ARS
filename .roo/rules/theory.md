# F3EO 理论白皮书 2.0

> 核心论断：F3EO 是一个**内在驱动的 Policy Gradient**，其奖励信号为 `-Tr(ℱ)`，更新规则为 `g - H·g`，旨在最小化变分自由能。  
> 本版本（2025-11-08）已修正 `batch_size=1` 的无偏性要求，并废弃所有与 `batch_size>1` 相关的旧描述。

---

## 1. 背景与动机

### 1.1 从「更快预训练」到「无奖励模型 RLHF」

F3E 系列最初的假设是“三阶优化 = 更快收敛”。然而，在 WikiText-2 上的实验表明，**F3EO-raw 在同等 step 数下并未显著优于 AdamW**。这迫使我们重新审视其理论定位。

新的核心是：**F3E 不是更快的 SGD，而是无需外部奖励模型的 RLHF**。  

- **对手**：PPO / DPO（需要偏好模型 + 多阶段训练）  
- **优势**：F3E 仅通过两次 `backward` 就内生地完成了探索/利用权衡，**单阶段、零额外数据**。

### 1.2 与 Policy Gradient 的类比

| 维度 | Policy Gradient (REINFORCE) | F3EO-raw (bs=1) |
|---|---|---|
| **奖励** | 外部回报 `R` | 内在复杂度 `-Tr(ℱ)` |
| **梯度** | `∇ log π · R` | `g - H·g` |
| **方差** | 高（单轨迹） | 高（单样本） |
| **无偏性** | 是 | 是（仅当 bs=1） |
| **平滑器** | GAE / PPO | Adam 动量 |

**结论**：F3E 是一个**内在驱动的 Policy Gradient**，其“策略”是模型本身，“奖励”是模型对世界的敏感度。
