## Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

> 原文: [arXiv:2505.06708](https://arxiv.org/abs/2505.06708)

### 摘要

该研究由阿里巴巴 Qwen 团队进行，系统性地探索了在标准 Softmax 注意力机制中引入门控单元的效果。通过在高达 15B 的 MoE 模型和 1.7B 的稠密模型上进行超过 30 种变体的详尽实验，研究得出了一个清晰而强大的结论。

**核心发现:**

1. **最优方案**: 在缩放点积注意力（SDPA）之后，应用一个**头特定 (head-specific) 的、乘法式的 Sigmoid 门控**，能够持续、显著地提升模型性能、训练稳定性，并改善模型的扩展属性。
2. **双重作用机制**: 该门控的有效性源于两大关键因素：
   - **注入非线性**: 打破了从值投影（$W_V$）到输出投影（$W_O$）之间存在的“低秩线性瓶颈”，增强了值通路的表达能力。
   - **引入稀疏性**: 门控产生了依赖于查询（Query）的**稀疏分数**，这为注意力头的输出提供了一个动态滤波器。正如您所指出的，这种调制方式在训练时提供了**一个更稀疏、更具信息量的注意力头更新信号**。
3. **消除注意力沉溺 (Attention Sink)**: 这种稀疏门控机制被证明能有效消除“注意力沉溺”现象——即注意力分数不合逻辑地过度集中于序列初始 Token（如 BOS）的问题，从而提升了模型在长上下文任务上的表现。

**研究意义:**

- 识别并提供了一个简单、高效的方案，以修正标准 Transformer 架构中一个长期被忽视的内在缺陷。
- 证明了在激活空间中进行动态、稀疏的调制是一种极具潜力的架构改进方向。
- 为训练更稳定、性能更强、上下文窗口更长的大语言模型提供了新的工程实践范式。

### 1. 核心机制: 后注意力门控 (Post-SDPA Gating)

该论文探索了在 Transformer 注意力层五个不同位置（G1-G5）引入门控的效果，并最终确定 G1 位置为最优。

**形式化定义:**

1. **标准注意力头输出**: $O_i = \text{Attention}(Q_i, K_i, V_i)$
2. **门控分数计算**: $g_i = \sigma(X_{prenorm} \cdot W_{\theta, i})$
   - $X_{prenorm}$ 是进入注意力层之前的隐藏状态。
   - $W_{\theta, i}$ 是为第 $i$ 个头专门学习的门控投影矩阵。
3. **门控后输出**: $\hat{O}_i = O_i \odot g_i$ (逐元素相乘)

该机制的核心是**自适应内容过滤**：每个注意力头根据输入内容，动态决定其输出信息的“价值”，并据此进行缩放。

### 2. 关键分析: 为何有效？

#### 2.1 非线性增强

论文指出，标准注意力中的 $W_V$ 和 $W_O$ 矩阵的连续应用，等价于一个单一的低秩线性映射 $W_V W_O$。在两者之间插入一个非线性的门控，极大地丰富了模型的表达能力。实验证明，仅在 G1（SDPA 之后）和 G2（Value 投影之后）位置插入门控能取得最好效果，这与该理论完全吻合。

#### 2.2 稀疏性与注意力沉溺

这是本文最核心的贡献。

- **稀疏门控**: 表现最好的门控变体，其门控分数分布都呈现出高度的稀疏性（大量值接近于 0）。
- **消除沉溺**: 基线模型中，平均 46.7% 的注意力集中在第一个 Token 上。引入 G1 门控后，该比例骤降至 4.8%。这意味着模型不再将注意力浪费在无信息的“沉溺点”上，而是更有效地分配到整个上下文。
- **更优的更新**: 通过过滤掉来自无效注意力的“噪声”，最终进入残差连接和反向传播的更新信号变得更稀疏、更具意义。这解释了为何训练过程更稳定，且能容忍更高的学习率。

### 3. 对 F3EO 研究的指导意义

1. **范式转移**: 我们的研究重心应从**纯粹的优化器设计**，转向**优化器与架构的协同设计**。为一个存在固有缺陷（如注意力沉溺）的架构设计再先进的优化器，效果可能都事倍功半。
2. **新基线确立**: 我们应立即将“后注意力门控”机制集成到我们的基线模型（如 `qwen3_rope.py`）中。一个消除了注意力沉溺的、“更健康”的架构，将为我们评估高级优化器（如 `AdaRMSuon`）的真实效果提供一个更公平、更可靠的平台。
3. **激活空间 vs. 参数空间**: 该研究为“激活空间记忆管理 (ASM)”提供了一个极其成功的范例。它表明，在引入更复杂的“参数空间记忆 (PSM)”之前，首先优化和完善 ASM 仍有巨大潜力。
