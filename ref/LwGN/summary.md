## 大型语言模型二阶优化潜力研究：基于全高斯-牛顿法的实验

> 原文长达 50k token 以上，若非必要，请避免直接阅读 `ref\The Potential of Second-Order Optimization for LLMs A Study with Full Gauss-Newton.md`。如果必须阅读原文，请发起 subtask，只回传摘要信息。

### 摘要

该研究探讨了在大型语言模型（LLMs）预训练中，全高斯-牛顿（GN）预处理对优化性能的潜在提升。现有方法多采用计算效率高的二阶近似，但其性能损失尚不明确。本研究通过对高达 1.5 亿参数的 Transformer 模型应用全高斯-牛顿预处理，为迭代复杂度设定了实际的上限。

**主要发现：**

1. **显著的迭代次数减少：** 全高斯-牛顿更新在迭代次数上显著优于现有优化器，与 SOAP 和 Muon 等强基线相比，训练迭代次数减少了 5.4 倍。
2. **层级高斯-牛顿的有效性：** 即使是忽略跨层信息的精确层级高斯-牛顿预处理器，其性能也几乎与全高斯-牛顿方法持平。
3. **性能差距的存在：** 当前的近似方法与理想化的层级预处理之间仍存在显著的性能差距。

**研究意义：**

- 高斯-牛顿近似在预处理方面非常有效，表明高阶损失项对于收敛速度可能并非关键。
- 层级 Hessian 结构包含足够的信息以实现大部分潜在性能提升。
- 为未来更实用的二阶优化方法设定了性能目标。

### 1. 引言

随着 LLMs 训练计算需求的增长，优化方法的改进成为提高训练效率的关键。传统的 LLM 优化主要依赖 Adam 等一阶方法。然而，近期研究开始探索二阶优化器，因其理论上更快的收敛速度和对更大批次规模的适应性。Shampoo、SOAP 和 Muon 等方法已显示出良好性能，但它们都采用 Hessian 矩阵的近似，以降低计算和内存成本。

本研究旨在回答：**LLMs 二阶优化的根本性能极限是什么？Hessian 的哪些结构特性对实现这些极限至关重要？**

为回答这些问题，研究首先通过全高斯-牛顿（GN）方法建立性能上限，并评估其迭代复杂度和对关键批次大小的影响。其次，通过比较全 GN 方法与两种变体（GN-prox-linear 和纯层级 GN）来分析 Hessian 的关键结构特性。

### 2. 现有优化器背景

- **Adam：** 最广泛使用的 LLM 优化器，维护梯度的一阶和二阶矩。
- **AdaGrad：** 维护梯度向量的累加器，并使用其逆平方根进行预处理。
- **Shampoo：** AdaGrad 的变体，可视为 Hessian 高斯-牛顿部分的近似，为权重矩阵的每个维度维护独立的预处理器。
- **SOAP：** Shampoo 的最新变体，在 Shampoo 提供的特征基中运行 AdamW。
- **Muon：** 跟踪梯度的一阶矩，并执行正交更新。

这些二阶方法在扩展到大批次规模时表现出有效性，但受限于计算和内存需求，通常采用每层预处理器的近似。

### 3. 全二阶优化

#### 3.1 高斯-牛顿矩阵

全二阶优化需要完全访问 Hessian 矩阵。然而，Hessian 不保证是半正定（PSD），可能导致不稳定更新。因此，通常使用高斯-牛顿（GN）矩阵，它捕获损失函数的曲率，但忽略模型的曲率，且对于 MSE 和交叉熵损失是 PSD 的，从而避免了不可靠的更新。

#### 3.2 内存可行的高斯-牛顿实现

由于直接计算 GN 矩阵不可行，本研究采用了一种功能等效的方法，利用雅可比-向量积（JVP）避免显式存储 Hessian。具体而言，通过内层优化器（本研究使用 Muon）最小化损失函数的二阶泰勒近似，该近似基于模型的一阶泰勒近似。

### 4. 实验细节

- **模型与数据集：** 在 C4 数据集上训练 45M 和 150M 参数的 LLaMA 模型。
- **基线：** AdamW、Muon 和 SOAP。
- **高斯-牛顿实现：** 在每个训练步骤中，对模型进行一阶泰勒近似，并对泰勒化模型上的交叉熵损失进行二阶泰勒近似。使用 Muon 作为“内层优化器”来最小化泰勒化损失，并通过线搜索更新模型参数。
- **GN-prox-linear：** 最小化线性化模型上的损失函数，以研究高阶损失项的影响。
- **优化策略：** 实验了不同的学习率调度（全局余弦、全局+内层余弦、常数+内层余弦）和正则化策略（内层和外层优化正则化、线搜索）。线搜索对于 GN 方法的稳定收敛至关重要。

### 5. 实验结果

#### 5.1 高斯-牛顿实验

- **迭代复杂度：** 在超大批次规模下（远超各方法临界批次），全高斯-牛顿方法达到目标损失 3.25 仅需 54 步，比 SOAP 快 5.4 倍，比 Muon 快 16 倍。这表明 GN 方法在大批次训练初期能快速收敛。
- **批次规模扩展：**
  - **固定 Token 数训练：** 在固定 Token 数（30 亿）训练下，GN 方法在批次规模超过 4M 时表现出显著优势，尤其在 1.2 亿批次规模下，仅用 20 步就达到 3.45 的损失，远优于 AdamW（损失超过 4.4）。
  - **达到目标验证损失所需的步数：** GN 方法在批次规模达到 40M 时仍持续减少达到目标损失所需的步数，表明其在大批次下具有更好的样本效率。AdamW 在 4M 批次规模后趋于平稳，SOAP 和 Muon 在 12M 后减少幅度减小。

#### 5.2 GN-prox-linear 方法

GN-prox-linear 方法（保留线性化模型上的完整损失函数而非二阶近似）的性能与全高斯-牛顿方法相似。这表明，在损失函数中包含高阶项对性能提升影响不大。

#### 5.3 层级高斯-牛顿

层级高斯-牛顿方法（忽略跨层曲率信息，独立优化每层）的性能几乎与全高斯-牛顿方法持平。在达到目标损失 3.25 时，层级 GN 仅比全 GN 多 1.4 倍的步骤，但仍比 SOAP 快 3.4 倍。这表明，即使仅使用层级曲率信息，也能实现显著的计算效率提升。

### 6. 讨论与结论

本研究通过对高达 1.5 亿参数的 LLaMA 模型应用全高斯-牛顿预处理，证明了精确二阶优化在 LLMs 训练中具有巨大潜力。尽管当前实现计算开销较大（比标准训练慢 4-5 倍），但它作为概念验证，揭示了二阶方法在收敛速度和批次规模扩展方面的显著优势。

**主要结论：**

1. **全高斯-牛顿预处理能显著减少 LLMs 训练的迭代次数，尤其在大批次规模下。**
2. **高阶损失项对性能提升并非关键，高斯-牛顿近似已足够有效。**
3. **层级 Hessian 结构包含足够信息，仅通过更好的层级近似即可实现大部分潜在性能提升。**

本研究为未来开发更实用、计算效率更高的二阶优化方法提供了明确的性能目标和研究方向，鼓励在计算效率和实用性方面进一步探索。

---

### 🔧 核心更新规则与工程实现细节

#### 1. 全 Gauss-Newton (GN) 更新规则

GN 方法的核心是求解一个**二次规划 (Quadratic Programming)** 问题，而非直接计算矩阵逆。

对于当前参数 `θ_t`，定义：

- `fθt(1)(θ,x) := f(θt,x) + ∇f(θt,x)ᵀ(θ - θt)`：模型的一阶泰勒展开。
- `ℒ~θt(2)(θ)`：在此线性化模型上的损失函数的二阶泰勒展开。

Gauss-Newton 更新 `θ*` 是以下最小二乘问题的解：

`θ* = argmin_θ ℒ~θt(2)(θ)`

该问题的闭式解为：

`θ* = θ_t - G⁻¹g`

其中：

- `g = ∇θℒ(f(θt,x),y)` 是当前梯度。
- `G = ∇θf(θt,x)ᵀ ∇z²ℒ(f(θt,x),y) ∇θf(θt,x)` 是 Gauss-Newton 矩阵（半正定）。

**工程实现**：为避免显式构造和求逆 `G`，论文采用**无矩阵 (Matrix-Free)** 方法：使用一个**内层优化器 (Inner Optimizer)** 来最小化 `ℒ~θt(2)(θ)`。具体步骤如下：

1. **构造二次损失**：利用 `neural-tangents` 库，基于当前批次数据，构建 `ℒ~θt(2)(θ)`。
2. **内层优化**：使用 Muon 优化器，在一个较小的内层批次 (`b_inner`) 上，对该二次损失进行 `N` 步优化。
3. **线搜索 (Line Search)**：将内层优化得到的参数更新应用于原始模型前，在 `{2^(-i/2) | i ∈ {0,...,4}}` 的步长集合上执行线搜索，以确保稳定收敛。

#### 2. 层级 Gauss-Newton (Layerwise GN) 更新规则

层级 GN 是对全 GN 的**块对角近似 (Block-Diagonal Approximation)**，忽略所有跨层曲率信息。其核心思想是**逐层独立求解**上述二次规划问题。

对于网络的每一层 `l`，定义：

- `θl,t`：层 `l` 在时刻 `t` 的参数。
- `fθl,t(1)(θl)`：模型 `f` 关于层 `l` 参数的一阶泰勒展开，其他层参数固定。

则层 `l` 的更新 `θl,t+1` 是以下独立问题的解：

`θl,t+1 = argmin_{θl} ℒ~θl,t(2)(θl)`

在所有层的更新计算完成后，将它们合并，并对整个网络的参数更新执行一次线搜索（步长集合更细：`i ∈ {0,...,9}`）。

**工程意义**：层级 GN 将一次大规模二次规划分解为多个小规模问题，**天然并行**，内存占用与层大小成正比，而非总参数量。实验表明，其性能与全 GN **几乎持平**（迭代次数仅多 1.4 倍），是实现 PI-Muon 中 `g_fisher` 的**首选工程方案**。

#### 3. 内层优化器与超参数配置

- **内层优化器**：Muon（论文实验显示其性能优于 AdamW）。
- **内层批次大小 (`b_inner`)**：32 (45M 模型) 或 128 (150M 模型)。
- **内层优化步数 (`N`)**：根据总批次大小 `b` 调整，满足 `b = N × b_inner`。
- **学习率调度**：对 GN 方法，“**constant+inner cosine**”调度（内层 Muon 用余弦衰减，外层步长恒定）在大批次下表现最佳。
- **正则化**：在内层损失中添加 `L2` 权重衰减（作用于参数更新量 `θ - θ_t`）以稳定训练。

---

### 📌 对 PI-Muon 实现的指导意义

1. **计算可行性**：层级 GN 的块对角结构为 PI-Muon 提供了**工程基石**。我们无需构造全局 `G`，只需对每层求解 `Gl x = gl`，天然支持分布式训练。
2. **内层优化器替换**：论文使用 Muon 作为内层优化器。在 PI-Muon 中，我们将此角色升级为**结构自然梯度 (`g_muon`)**，并通过**正交投影**将其与 LwGN 的统计自然梯度融合，形成更稳健的更新方向。
3. **线搜索与调度**：PI-Muon 可复用论文中的线搜索策略和“constant+inner cosine”调度，但融合权重 `λ_t` 由 PI 动态调控，**取代**了固定的线搜索步长选择，实现了**自适应步长**的效果。
